<tool id="llm_hub" name="LLM Hub" version="@TOOL_VERSION@+galaxy@VERSION_SUFFIX@" profile="@PROFILE@">
    <description>Call any LLM</description>
    <macros>
        <import>macros.xml</import>
        <token name="@VERSION_SUFFIX@">0</token>
        <token name="@PROFILE@">24.0</token>
    </macros>
    <requirements>
        <requirement type="package" version="3.12">python</requirement>
        <requirement type="package" version="6.0.3">pyyaml</requirement>
        <requirement type="package" version="@TOOL_VERSION@">openai</requirement>
    </requirements>
    <command detect_errors="exit_code"><![CDATA[
#import json
#import os
#import re

#set LINK_LIST = []
#for $input_type, $param in [('text', 'text_context'), ('image', 'image_context')]
    #set context = $getVar($param, None)
    #if $model_type in [$input_type, 'multimodal'] and $context
        #for $input in ($context if isinstance($context, list) else [$context])
            #set file_name = re.sub('[^\w\-]', '_', os.path.splitext($input.element_identifier)[0])
            #set link_name = '%s_%s.%s' % ($input.hid, $file_name, $input.ext)
            ln -s '$input' '$link_name' &&
            $LINK_LIST.append(($link_name, $input_type))
        #end for
    #end if
#end for
#set context_files = json.dumps($LINK_LIST)

#if $model_type == 'image'
    #set prompt = ''
#end if

python '$__tool_directory__/llm_hub.py'
'$context_files'
'$prompt'
'$model.fields.value'
'$model_type'
    ]]></command>
    <inputs>
        <conditional name="model_condition">
            <param name="model_type" type="select" label="Choose the model" help="Multimodal models are capable to have image and text as input.">
                <option value="multimodal" selected="true">Multimodal models</option>
                <option value="text">Text models</option>
                <option value="image">Image models</option>
            </param>
            <when value="multimodal">
                <param name="model" type="select" optional="false" label="Model" help="Select the model you want to use.">
                    <options from_data_table="llm_models">
                        <filter type="static_value" column="2" value="multimodal"/>
                    </options>
                    <validator message="No model annotation is available for LLM Hub" type="no_options"/>
                </param>
                <param name="text_context" type="data" multiple="true" optional="true" format="html,json,txt" label="Text Context"/>
                <param name="image_context" type="data" optional="true" format="jpg,png,gif,tiff,bmp" label="Image Context"/>
                <param name="prompt" type="text" optional="false" label="Prompt" help="Prompts or tasks you want the LLM to perform." area="true">
                    <validator type="empty_field"/>
                </param>
            </when>
            <when value="text">
                <param name="model" type="select" optional="false" label="Model" help="Select the model you want to use.">
                    <options from_data_table="llm_models">
                        <filter type="static_value" column="2" value="text"/>
                    </options>
                    <validator message="No model annotation is available for LLM Hub" type="no_options"/>
                </param>
                <param name="text_context" type="data" multiple="true" optional="true" format="html,json,txt" label="Text Context"/>
                <param name="prompt" type="text" optional="false" label="Prompt" help="Prompts or tasks you want the LLM to perform." area="true">
                    <validator type="empty_field"/>
                </param>
            </when>
            <when value="image">
                <param name="model" type="select" optional="false" label="Model" help="Select the model you want to use.">
                    <options from_data_table="llm_models">
                        <filter type="static_value" column="2" value="image"/>
                    </options>
                    <validator message="No model annotation is available for LLM Hub" type="no_options"/>
                </param>
                <param name="image_context" type="data" optional="false" format="jpg,png,gif,tiff,bmp" label="Image Context"/>
            </when>
        </conditional>
    </inputs>
    <outputs>
        <data name="output" format="markdown" label="${tool.name}(${model}) #if $on_string then ' on ' + $on_string else ''#" from_work_dir="./output.md"/>
    </outputs>
    <tests>
        <test expect_failure="true" expect_exit_code="1">
            <conditional name="model_condition">
                <param name="model_type" value="text"/>
                <param name="model" value="unknown"/>
                <param name="text_context" value="test.txt" ftype="txt"/>
                <param name="prompt" value="What is this?"/>
            </conditional>
            <assert_stdout>
                <has_text text="LiteLLM API key is not configured!"/>
            </assert_stdout>
        </test>
    </tests>
    <help><![CDATA[

.. class:: infomark

**What it does**

This tool leverages LiteLLM proxy to access various Large Language Models (LLMs) for generating responses based on user-provided context and prompts.
Users can upload context data in various formats and ask questions or execute prompts related to that data.
The tool connects to a LiteLLM proxy server configured by Galaxy administrators, which can provide access to multiple LLM providers including OpenAI, Anthropic, Google, and others.

When you run this tool, your input data is processed by the selected LLM through the configured LiteLLM proxy. 
The models process the data and generate a response based on the context and prompt provided. 
After receiving the response from the LLM, the tool returns it to Galaxy and puts it in your history.

Usage
.....

**Input**

1. **Select a Model**: Choose the LLM model that best fits your needs. 
Available models depend on what's configured in the LiteLLM proxy by your Galaxy administrators.

2. **Upload Context Data**: You can upload context data in different ways depending on the model type:

   - **Text models**: Upload multiple text files (TXT, HTML, JSON) as context
   - **Image models**: Upload a single image file (JPG, PNG, GIF, TIFF, BMP) as context  
   - **Multimodal models**: Upload multiple text files and/or a single image file as context

   Vision-capable (multimodal and image) models can process image files, but only one image file is supported per request.

3. **Provide a Prompt**: Specify the task or question you want the LLM to address.
The more specific the prompt, the more tailored the response will be.

**Output**

The output is a response generated by the selected LLM, crafted based on the provided context data and the prompt.
This response is saved in a Markdown file.

    ]]></help>
    <citations>
        <citation type="bibtex">
            @misc{litellm,
                author = {LiteLLM},
                title = {LiteLLM - Call all LLM APIs using the OpenAI format},
                howpublished = {\url{https://docs.litellm.ai/}},
                year = {2025}
            }
        </citation>
    </citations>
</tool>
