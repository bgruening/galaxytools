<tool id="llm_hub" name="LLM Hub" version="@TOOL_VERSION@+galaxy@VERSION_SUFFIX@" profile="@PROFILE@">
    <description>Call any LLM</description>
    <macros>
        <import>macros.xml</import>
        <token name="@VERSION_SUFFIX@">1</token>
        <token name="@PROFILE@">24.0</token>
    </macros>
    <requirements>
        <requirement type="package" version="3.12">python</requirement>
        <requirement type="package" version="6.0.2">pyyaml</requirement>
        <requirement type="package" version="@TOOL_VERSION@">openai</requirement>
    </requirements>
    <command detect_errors="exit_code"><![CDATA[
#import json
#import os
#import re

#set LINK_LIST = []
#for $input_type, $param in [('text', 'text_context'), ('image', 'image_context')]
    #set context = $getVar($param, None)
    #if $model_type in [$input_type, 'multimodal'] and $context
        #for $input in ($context if isinstance($context, list) else [$context])
            #set file_name = re.sub('[^\w\-]', '_', os.path.splitext($input.element_identifier)[0])
            #set link_name = '%s_%s.%s' % ($input.hid, $file_name, $input.ext)
            ln -s '$input' '$link_name' &&
            $LINK_LIST.append(($link_name, $input_type))
        #end for
    #end if
#end for
#set context_files = json.dumps($LINK_LIST)

#if $model_type == 'image'
    #set prompt = ''
#end if

python '$__tool_directory__/llm_hub.py'
'$context_files'
'$prompt'
'$model.fields.model_id'
'$model_type'
'$advanced.temperature'
'$model.fields.provider'
    ]]></command>
    <inputs>
        <conditional name="model_condition">
            <param name="model_type" type="select" label="Choose the model" help="Multimodal models are capable to have image and text as input.">
                <option value="multimodal" selected="true">Multimodal models</option>
                <option value="text">Text models</option>
                <option value="image">Image models</option>
            </param>
            <when value="multimodal">
                <param name="model" type="select" optional="false" label="Model" help="Select the model you want to use.">
                    <options from_data_table="llm_models">
                        <filter type="static_value" column="3" value="multimodal"/>
                    </options>
                    <validator message="No model annotation is available for LLM Hub" type="no_options"/>
                </param>
                <param name="text_context" type="data" multiple="true" optional="true" format="html,json,txt" label="Text Context"/>
                <param name="image_context" type="data" optional="true" format="jpg,png,gif,tiff,bmp" label="Image Context"/>
                <param name="prompt" type="text" optional="false" label="Prompt" help="Prompts or tasks you want the LLM to perform." area="true">
                    <validator type="empty_field"/>
                </param>
            </when>
            <when value="text">
                <param name="model" type="select" optional="false" label="Model" help="Select the model you want to use.">
                    <options from_data_table="llm_models">
                        <filter type="static_value" column="3" value="text"/>
                    </options>
                    <validator message="No model annotation is available for LLM Hub" type="no_options"/>
                </param>
                <param name="text_context" type="data" multiple="true" optional="true" format="html,json,txt" label="Text Context"/>
                <param name="prompt" type="text" optional="false" label="Prompt" help="Prompts or tasks you want the LLM to perform." area="true">
                    <validator type="empty_field"/>
                </param>
            </when>
            <when value="image">
                <param name="model" type="select" optional="false" label="Model" help="Select the model you want to use.">
                    <options from_data_table="llm_models">
                        <filter type="static_value" column="3" value="image"/>
                    </options>
                    <validator message="No model annotation is available for LLM Hub" type="no_options"/>
                </param>
                <param name="image_context" type="data" optional="false" format="jpg,png,gif,tiff,bmp" label="Image Context"/>
            </when>
        </conditional>
        <section name="advanced" title="Advanced Options" expanded="false">
            <param name="temperature" type="float" optional="true" min="0.0" max="2.0" label="Temperature" help="Controls randomness in the output. Lower values make output more focused and deterministic, while higher values make it more creative and random. If not set, the model's default temperature will be used."/>
        </section>
    </inputs>
    <outputs>
        <data name="output" format="markdown" label="${tool.name} (${model}) #if $on_string then ' on ' + $on_string else ''#" from_work_dir="./output.md"/>
    </outputs>
    <tests>
        <test expect_failure="true" expect_exit_code="1">
            <conditional name="model_condition">
                <param name="model_type" value="text"/>
                <param name="model" value="unknown"/>
                <param name="text_context" value="test.txt" ftype="txt"/>
                <param name="prompt" value="What is this?"/>
            </conditional>
            <section name="advanced">
                <param name="temperature" value="0.7"/>
            </section>
            <assert_stdout>
                <has_text text="LiteLLM API key is not configured!"/>
            </assert_stdout>
        </test>
    </tests>
    <help><![CDATA[

.. class:: infomark

**What it does**

This tool leverages LiteLLM proxy to access various Large Language Models (LLMs) for generating responses based on user-provided context and prompts.
Users can upload context data in various formats and ask questions or execute prompts related to that data.
The tool connects to a LiteLLM proxy server configured by Galaxy administrators, which can provide access to multiple LLM providers including OpenAI, Anthropic, Google, and others.

When you run this tool, your input data is processed by the selected LLM through the configured LiteLLM proxy. 
The models process the data and generate a response based on the context and prompt provided. 
After receiving the response from the LLM, the tool returns it to Galaxy and puts it in your history.

Usage
.....

**Input**

1. **Select a Model**: Choose the LLM model that best fits your needs. 
Available models depend on what's configured in the LiteLLM proxy by your Galaxy administrators.

2. **Upload Context Data**: You can upload context data in different ways depending on the model type:

   - **Text models**: Upload multiple text files (TXT, HTML, JSON) as context
   - **Image models**: Upload a single image file (JPG, PNG, GIF, TIFF, BMP) as context  
   - **Multimodal models**: Upload multiple text files and/or a single image file as context

   Vision-capable (multimodal and image) models can process image files, but only one image file is supported per request.

3. **Provide a Prompt**: Specify the task or question you want the LLM to address.
The more specific the prompt, the more tailored the response will be.

**Advanced Options**

- **Temperature**: An optional parameter that controls the randomness of the model's output (range: 0.0 to 2.0).
  
  - **Lower values** (e.g., 0.0-0.3): Make the output more focused, consistent, and deterministic. Use for tasks requiring factual accuracy or consistent formatting.
  - **Higher values** (e.g., 0.7-2.0): Make the output more creative, diverse, and random. Use for brainstorming, creative writing, or varied responses.
  - **Not set**: When left empty, the model uses its default temperature setting, which is typically optimized for general-purpose tasks.

**Output**

The output is a response generated by the selected LLM, crafted based on the provided context data and the prompt.
This response is saved in a Markdown file.

    ]]></help>
    <citations>
        <citation type="bibtex">
            @misc{litellm,
                author = {LiteLLM},
                title = {LiteLLM - Call all LLM APIs using the OpenAI format},
                howpublished = {\url{https://docs.litellm.ai/}},
                year = {2025}
            }
        </citation>
    </citations>
</tool>
