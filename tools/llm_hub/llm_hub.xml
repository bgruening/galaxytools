<tool id="llm_hub" name="LLM Hub" version="@TOOL_VERSION@+galaxy@VERSION_SUFFIX@" profile="@PROFILE@">
    <description>Call any LLM</description>
    <macros>
        <import>macros.xml</import>
        <token name="@VERSION_SUFFIX@">1</token>
        <token name="@PROFILE@">24.0</token>
    </macros>
    <requirements>
        <requirement type="package" version="3.12">python</requirement>
        <requirement type="package" version="6.0.2">pyyaml</requirement>
        <requirement type="package" version="@TOOL_VERSION@">openai</requirement>
    </requirements>
    <command detect_errors="exit_code"><![CDATA[
#import json
#import os
#import re
#set LINK_LIST = []
#for $input in $context
    #set file_name = os.path.splitext($input.element_identifier)[0]
    #set ext = $input.ext if $input.ext in ['html', 'json', 'txt', 'jpg', 'jpeg', 'png', 'gif'] else 'txt'
    #set LINK = re.sub('[^\w\-]', '_', $file_name)+'.'+$ext
    ln -s '$input' '$LINK' &&
    #set type = 'image' if $input.ext in ['jpg', 'jpeg', 'png', 'gif'] else 'text'
    $LINK_LIST.append([$LINK, $type])
#end for
#set context_files = json.dumps($LINK_LIST)
#if str($input_type.input_type_selector) == 'image'
    #set prompt = ''
#end if

python '$__tool_directory__/llm_hub.py'
'$context_files'
'$prompt'
'$model.fields.value'
'$input_type_selector'
    ]]></command>
    <inputs>
        <conditional name="input_type">
            <param name="input_type_selector" type="select" label="Choose the model" help="Multimodal models are capable to have image and text as input.">
                <option value="multimodal" selected="true">Multimodal models</option>
                <option value="text">Text models</option>
                <option value="image">Image models</option>
            </param>
            <when value="multimodal">
                <param name="model" type="select" optional="false" label="Model" help="Select the model you want to use.">
                    <options from_data_table="llm_models">
                        <filter type="static_value" column="2" value="multimodal"/>
                    </options>
                    <validator message="No model annotation is available for LLM Hub" type="no_options"/>
                </param>
                <param name="context" type="data" multiple="true" optional="true" format="html,json,txt,jpg,png,gif" label="Context" max="500"/>
                <param name="prompt" type="text" optional="false" label="Prompt" help="Prompts or tasks you want the LLM to perform." area="true">
                    <validator type="empty_field"/>
                </param>
            </when>
            <when value="text">
                <param name="model" type="select" optional="false" label="Model" help="Select the model you want to use.">
                    <options from_data_table="llm_models">
                        <filter type="static_value" column="2" value="text"/>
                    </options>
                    <validator message="No model annotation is available for LLM Hub" type="no_options"/>
                </param>
                <param name="context" type="data" multiple="true" optional="true" format="html,json,txt" label="Context" max="500"/>
                <param name="prompt" type="text" optional="false" label="Prompt" help="Prompts or tasks you want the LLM to perform." area="true">
                    <validator type="empty_field"/>
                </param>
            </when>
            <when value="image">
                <param name="model" type="select" optional="false" label="Model" help="Select the model you want to use.">
                    <options from_data_table="llm_models">
                        <filter type="static_value" column="2" value="image"/>
                    </options>
                    <validator message="No model annotation is available for LLM Hub" type="no_options"/>
                </param>
                <param name="context" type="data" multiple="true" optional="false" format="jpg,png,gif" label="Context" max="500"/>
            </when>
        </conditional>
    </inputs>
    <outputs>
        <data name="output" format="markdown" label="${tool.name} on ${on_string}" from_work_dir="./output.md"/>
    </outputs>
    <tests>
        <test expect_failure="true" expect_exit_code="1">
            <conditional name="input_type">
                <param name="input_type_selector" value="text"/>
                <param name="model" value="unknown"/>
                <param name="context" value="test.txt" ftype="txt"/>
                <param name="prompt" value="What is this?"/>
            </conditional>
            <assert_stdout>
                <has_text text="LiteLLM API key is not configured!"/>
            </assert_stdout>
        </test>
    </tests>
    <help><![CDATA[

.. class:: infomark

**What it does**

This tool leverages LiteLLM proxy to access various Large Language Models (LLMs) for generating responses based on user-provided context and prompts.
Users can upload context data in various formats and ask questions or execute prompts related to that data.
The tool connects to a LiteLLM proxy server configured by Galaxy administrators, which can provide access to multiple LLM providers including OpenAI, Anthropic, Google, and others.

When you run this tool, your input data is processed by the selected LLM through the configured LiteLLM proxy. 
The models process the data and generate a response based on the context and prompt provided. 
After receiving the response from the LLM, the tool returns it to Galaxy and puts it in your history.

Usage
.....

**Input**

1. **Select a Model**: Choose the LLM model that best fits your needs. 
Available models depend on what's configured in the LiteLLM proxy by your Galaxy administrators.

2. **Upload Context Data**: You can upload files in formats such as TXT, HTML, JSON, JPG, PNG, or GIF. 
This context data serves as additional input for the prompt you wish to execute.
Vision-capable models can process image files.

3. **Provide a Prompt**: Specify the task or question you want the LLM to address.
The more specific the prompt, the more tailored the response will be.

**Output**

The output is a response generated by the selected LLM, crafted based on the provided context data and the prompt.
This response is saved in a Markdown file.

    ]]></help>
    <citations>
        <citation type="bibtex">
            @misc{litellm,
                author = {LiteLLM},
                title = {LiteLLM - Call all LLM APIs using the OpenAI format},
                howpublished = {\url{https://docs.litellm.ai/}},
                year = {2025}
            }
        </citation>
    </citations>
</tool>
